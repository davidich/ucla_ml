{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Boston House-Prices\n",
    "* Q: _Which one gets better results? If you were not able to implement one of them, which one you suspect should get better results and why?_  \n",
    "A: KNN gets better result because linear regression underfits the data\n",
    "* Q: _Which measures are you using to compare them?_  \n",
    "A: Mean Squared Error\n",
    "* Q: _Which one is faster during training? And during inference time?_  \n",
    "A: KNN is faster during traning. Linear regression is faster during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Breast Cancer Wisconsin\n",
    "\n",
    "* Q: _Which one gets better results? If you were not able to implement one of them, which one you suspect should get better results and why?_  \n",
    "A: Logistic regression gets better results, because it builds precise decision boundary and KNN is confused a lot in that zone.\n",
    "* Q: _Which measures are you using to compare them? Which algorithm has more False Positives? And False Negatives? If you were not able to implement one of them, explain the concept of False Positives and False Negatives._  \n",
    "A: I used F1 score and confusion maxtix. Logistic regression performed better. It had 1 False Positive and 1 False Negative vs 5 and 2 for KNN.\n",
    "* Q: _If you were a doctor, which algorithm would you use for the prediction? If you were not able to implement one of them, explain how you would use a confusion matrix to answer this question._  \n",
    "A: I would use the algoritm with the least False Negative results. In my case that was Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Using Scikit-learn and comparing results\n",
    "\n",
    "* Q: _Which one gets better results? Please specify per dataset._  \n",
    "A: My Linear regression performed a little bit better, because by default it doesn't have regularization and better fits the data. But it was slower as I implemented batch gradient descent which is less efficient in terms of learning time.  \n",
    "Sci-kit's Logistic regression performed better, because my gradient decent had hard time finding precise coefficients.  \n",
    "* Q: _How could you modify the other methods to achieve similar results?_\n",
    "A: For linear regression I need to enable regularization and for logistic regression I need to change algorithm for finding more precise coefficients.  \n",
    "* Q: _Which one is faster during training? And during inference time?_  \n",
    "A: Sci-kit learn is faster during traning and they are identical during inference.  \n",
    "* Q: _Using the methods from Scikit-Learn, play around with the parameters. What is the best result you can get? What parameter was the most important for each dataset?_  \n",
    "A: For both dataset the most important was max_iterations and learning_rate. Penalty and alpha also influence the score, but only in a negative way.  \n",
    "* Q: _(Bonus) For Linear Regression, plot the coefficients after training models with L1 and L2 regularization. How do they differ?_  \n",
    "A: L1 removes unimportant features, L2 makes coefficients smaller.  \n",
    "* Q: _(Bonus) For Logistic Regression, plot the coefficients after training models with L1 and L2 regularization. How do they differ?_  \n",
    "A: L1 removes unimportant features, L2 makes coefficients smaller.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 How long did it take? \n",
    "40 hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
